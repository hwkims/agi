import streamlit as st
import requests
import json
import time # Optional: for simulating typing effect

# --- Page Configuration ---
st.set_page_config(page_title="Gemma3 API Chat", page_icon="ğŸ§ ", layout="wide")

st.title("ğŸ§  Gemma3 ê¸°ë°˜ 'AGI' ì±—ë´‡ (API ì§ì ‘ í˜¸ì¶œ)")
st.caption("ë¡œì»¬ Ollama APIë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ Gemma3 ëª¨ë¸ê³¼ ëŒ€í™”í•©ë‹ˆë‹¤. í•œêµ­ì–´ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

# --- Ollama API Configuration ---
st.sidebar.header("Ollama API ì„¤ì •")
ollama_api_base_url = st.sidebar.text_input("Ollama API Base URL", "http://localhost:11434")
OLLAMA_API_CHAT_ENDPOINT = f"{ollama_api_base_url}/api/chat"
OLLAMA_API_TAGS_ENDPOINT = f"{ollama_api_base_url}/api/tags"

# --- Model Selection (using API) ---
available_models = []
error_message = ""
try:
    response = requests.get(OLLAMA_API_TAGS_ENDPOINT, timeout=5) # 5ì´ˆ íƒ€ì„ì•„ì›ƒ
    response.raise_for_status() # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
    models_data = response.json()
    available_models = [m['name'] for m in models_data.get('models', []) if 'gemma3' in m['name']]
    if not available_models:
        error_message = "Ollamaì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ Gemma3 ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'ollama run gemma3:...' ëª…ë ¹ì–´ë¡œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”."
except requests.exceptions.RequestException as e:
    error_message = f"Ollama API({ollama_api_base_url}) ì—°ê²° ì‹¤íŒ¨: {e}. Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”."
except json.JSONDecodeError:
    error_message = "Ollama API ì‘ë‹µì„ íŒŒì‹±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
except Exception as e:
    error_message = f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}"

if error_message:
    st.sidebar.error(error_message)
    # Fallback list if API call fails or no gemma3 models found
    available_models = ["gemma3:4b", "gemma3:12b", "gemma3:27b", "gemma3:1b"]
    selected_model = st.sidebar.selectbox(
        "ì‹¤í–‰í•  Gemma3 ëª¨ë¸ ì„ íƒ (Fallback):",
        options=available_models,
        index=0,
        help=f"Ollama API ì—°ê²° ì‹¤íŒ¨ ë˜ëŠ” Gemma3 ëª¨ë¸ ë¶€ì¬. ê¸°ë³¸ ëª©ë¡ ì‚¬ìš©. ({error_message})"
    )
    ollama_ready = False
else:
     # Try to default to a larger model if available
    default_index = 0
    preferred_models = ["gemma3:27b", "gemma3:12b", "gemma3:4b", "gemma3:1b"]
    for i, model in enumerate(preferred_models):
        if model in available_models:
            default_index = available_models.index(model)
            break

    selected_model = st.sidebar.selectbox(
        "ì‹¤í–‰í•  Gemma3 ëª¨ë¸ ì„ íƒ:",
        options=available_models,
        index=default_index,
        help="ë¡œì»¬ Ollama APIì—ì„œ ì¡°íšŒëœ Gemma3 ëª¨ë¸ ëª©ë¡ì…ë‹ˆë‹¤."
    )
    ollama_ready = True

# --- Chat History Management ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# Add a system prompt
if not st.session_state.messages:
     st.session_state.messages.append(
         {"role": "system",
          "content": "ë‹¹ì‹ ì€ ë§¤ìš° ì§€ëŠ¥ì ì´ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì´ë¦„ì€ 'ì§€í˜œ'ì…ë‹ˆë‹¤. í•­ìƒ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì™„ë²½í•˜ê²Œ ê¸°ì–µí•˜ê³  í™œìš©í•˜ì—¬ ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”. ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ê³  ë°œì „í•˜ëŠ” ëª¨ìŠµì„ ë³´ì—¬ì£¼ë ¤ê³  ë…¸ë ¥í•˜ì„¸ìš”."}
     )

# --- Display Chat Messages ---
for message in st.session_state.messages:
    if message["role"] != "system":
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# --- Handle User Input and Ollama API Interaction ---
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”... (ì˜ˆ: 'ì•ˆë…•, ì§€í˜œì•¼?')"):
    if not ollama_ready:
         st.error("Ollama APIì— ì—°ê²°í•  ìˆ˜ ì—†ê±°ë‚˜ ëª¨ë¸ì„ ì„ íƒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë©”ì‹œì§€ë¥¼ ë³´ë‚¼ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    elif not selected_model:
        st.error("ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
    else:
        # 1. Add user message to session state and display it
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # 2. Call Ollama API
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            try:
                # Prepare the payload for the API request
                payload = {
                    "model": selected_model,
                    "messages": st.session_state.messages, # Send the whole history for context
                    "stream": True, # Use streaming response
                    "options": { # Optional parameters
                        'temperature': 0.7,
                        'top_k': 50,
                        'top_p': 0.9,
                        # 'stop': ['<end_of_turn>'] # Define stop tokens if needed by model
                    }
                }

                # Make the POST request with streaming enabled
                response = requests.post(
                    OLLAMA_API_CHAT_ENDPOINT,
                    json=payload,
                    stream=True,
                    headers={"Content-Type": "application/json"},
                    timeout=120 # Set a timeout for the request (e.g., 120 seconds)
                )
                response.raise_for_status() # Check for HTTP errors (like 404, 500)

                # Process the streaming response line by line
                for line in response.iter_lines():
                    if line:
                        try:
                            # Each line is a JSON object, decode it
                            chunk_str = line.decode('utf-8')
                            chunk_json = json.loads(chunk_str)

                            # Extract the content part from the message
                            if 'message' in chunk_json and 'content' in chunk_json['message']:
                                content_piece = chunk_json['message']['content']
                                full_response += content_piece
                                # Simulate typing effect by updating the placeholder
                                message_placeholder.markdown(full_response + "â–Œ")
                                time.sleep(0.01) # Small delay for effect

                            # Check if the stream is done (optional, depends on how Ollama signals end)
                            if chunk_json.get('done'):
                                break

                        except json.JSONDecodeError:
                            st.warning(f"ì‘ë‹µ ìŠ¤íŠ¸ë¦¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¹„-JSON ë¼ì¸ ë¬´ì‹œ): {line}")
                        except Exception as chunk_e:
                            st.warning(f"ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {chunk_e}")


                # Display the final full response without the cursor
                message_placeholder.markdown(full_response)

                # 3. Add assistant response to session state
                if full_response:
                    st.session_state.messages.append({"role": "assistant", "content": full_response})

            except requests.exceptions.RequestException as e:
                error_text = f"Ollama API ({OLLAMA_API_CHAT_ENDPOINT}) í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
                st.error(error_text)
                message_placeholder.markdown(f"ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ({e})")
            except Exception as e:
                st.error(f"ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
                message_placeholder.markdown(f"ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ({e})")

# --- Sidebar Options ---
st.sidebar.markdown("---")
if st.sidebar.button("ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"):
    st.session_state.messages = []
    # Re-add the system prompt after clearing
    st.session_state.messages.append(
         {"role": "system",
          "content": "ë‹¹ì‹ ì€ ë§¤ìš° ì§€ëŠ¥ì ì´ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì´ë¦„ì€ 'ì§€í˜œ'ì…ë‹ˆë‹¤. í•­ìƒ ì¹œì ˆí•˜ê³  ìƒì„¸í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì™„ë²½í•˜ê²Œ ê¸°ì–µí•˜ê³  í™œìš©í•˜ì—¬ ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”. ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ê³  ë°œì „í•˜ëŠ” ëª¨ìŠµì„ ë³´ì—¬ì£¼ë ¤ê³  ë…¸ë ¥í•˜ì„¸ìš”."}
     )
    st.rerun() # Rerun the script to refresh the page

st.sidebar.markdown("---")
st.sidebar.markdown("Powered by [Ollama API](https://github.com/ollama/ollama/blob/main/docs/api.md) and [Streamlit](https://streamlit.io)")
st.sidebar.markdown(f"í˜„ì¬ ëª¨ë¸: **{selected_model if selected_model else 'ì„ íƒ ì•ˆë¨'}**")
st.sidebar.markdown(f"API ìƒíƒœ: **{'ì—°ê²°ë¨' if ollama_ready else 'ì—°ê²° ì‹¤íŒ¨'}**")
