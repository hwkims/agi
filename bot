import streamlit as st
import requests
import base64
import json
import io
import asyncio
from edge_tts import Communicate
import logging
# from PIL import Image  # 이미지 처리 আপাতত 주석 처리
import streamlit.components.v1 as components

# 로깅 설정
logging.basicConfig(level=logging.INFO)

# Ollama 서버 주소
OLLAMA_HOST = 'http://localhost:11434'
# 음성 설정
VOICE = "ko-KR-HyunsuNeural"

# 시스템 프롬프트 (Few-shot 예시 제거)
SYSTEM_PROMPT = """
... (이전과 동일) ...
"""

async def tts(text, voice=VOICE):
    """Edge TTS를 사용하여 텍스트를 음성으로 변환"""
    try:
        communicate = Communicate(text, voice)
        audio_data = io.BytesIO()
        async for chunk in communicate.stream():
            if chunk["type"] == "audio":
                audio_data.write(chunk["data"])
        audio_data.seek(0)
        return audio_data

    except Exception as e:
        logging.error(f"TTS Error: {e}")
        st.error(f"TTS 오류: {e}")
        return None


def query_ollama(prompt, context=None, image_data=None):
    """Ollama API 호출 (대화 및 이미지)"""
    data = {
        "model": "gemma3:4b",  # Gemma 4B 모델 사용 (모델 이름 확인 필요)
        "prompt": prompt,
        "stream": False,
        "context": context or [],
        "options": {"temperature": 0.2, "top_p": 0.8},
        "format": "json" # Ollama에 직접 JSON 형식 요청
    }
    if image_data:
        data["images"] = [image_data]

    try:
        response = requests.post(f"{OLLAMA_HOST}/api/generate", json=data, stream=False, timeout=60)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        logging.error(f"Ollama Request Error: {e}")
        return {"error": f"Ollama API 요청 오류: {e}"}

async def main():
    st.title("AI 대화 챗봇 (이미지 + 음성 인식)")

    # 세션 상태 초기화
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "context" not in st.session_state:
        st.session_state.context = []
    if "speech_active" not in st.session_state:
        st.session_state.speech_active = False
    if "last_json" not in st.session_state:
        st.session_state.last_json = None
    if "recognition" not in st.session_state:
        st.session_state.recognition = None

    # 대화 기록 표시
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if "audio" in message:
                st.audio(message["audio"])

    # # 이미지 업로드 (주석 처리)
    # uploaded_file = st.file_uploader("이미지를 업로드하세요", type=["jpg", "jpeg", "png"])
    # img_base64 = None
    # if "uploaded_file" not in st.session_state:
    #     st.session_state.uploaded_file = None
    # if uploaded_file is not None:
    #     image = Image.open(uploaded_file)
    #     st.image(image, caption="업로드된 이미지", use_column_width=True)
    #     buffered = io.BytesIO()
    #     image.save(buffered, format="JPEG")
    #     img_base64 = base64.b64encode(buffered.getvalue()).decode()
    #     st.session_state.uploaded_file = img_base64  # 이미지 저장
    img_base64 = None # 이미지 관련 변수 초기화
    # 음성 인식 활성화/비활성화 버튼
    if st.button("음성 인식 " + ("켜기" if not st.session_state.speech_active else "끄기")):
        st.session_state.speech_active = not st.session_state.speech_active
        if st.session_state.speech_active:
            st.write("음성 인식이 활성화되었습니다. 마이크에 말씀하세요.")
        else:
            st.write("음성 인식이 비활성화되었습니다.")


    # 사용자 입력 (텍스트) 처리 + 음성 인식 결과 처리
    user_input = st.chat_input("무엇이든 물어보세요")
    if "user_input_from_speech" not in st.session_state:
        st.session_state.user_input_from_speech = ""

    if user_input or st.session_state.user_input_from_speech:
        # 텍스트 입력 우선 처리
        if user_input:
            final_user_input = user_input
            st.session_state.user_input_from_speech = ""  # 텍스트 입력이 있으면 음성 입력 초기화
        else:
            final_user_input = st.session_state.user_input_from_speech
            st.session_state.user_input_from_speech = ""


        st.session_state.messages.append({"role": "user", "content": final_user_input})
        with st.chat_message("user"):
            st.markdown(final_user_input)



        # Ollama에 쿼리 (이미지/텍스트) - 이미지 데이터 없이 텍스트만
        with st.spinner("답변 생성 중..."):
            ollama_response = query_ollama(
                f"{SYSTEM_PROMPT}\n\n[INST] text: {final_user_input}[\INST]",
                st.session_state.context,
                image_data=None,  # 이미지 데이터 없음
            )

        if "error" in ollama_response:
            st.error(ollama_response["error"])
            assistant_response_content = "죄송해요, 무슨 말씀인지 잘 모르겠어요."
            json_output = None
        else:
            try:
                json_output = json.loads(ollama_response["response"])
                assistant_response_content = json_output["speech_output"]
                st.session_state.context = ollama_response.get('context', [])
                st.session_state.last_json = json_output
            except json.JSONDecodeError:
                st.error("Ollama 모델로부터 유효한 JSON 응답을 받지 못했습니다.")
                assistant_response_content = "죄송합니다. 응답을 처리하는 중에 오류가 발생했습니다."
                json_output = None

        with st.chat_message("assistant"):
            st.markdown(assistant_response_content)
            if json_output:
                with st.expander("JSON 출력 (디버깅)"):
                    st.json(json_output)
            with st.spinner("음성 생성 중..."):
                audio_data = await tts(assistant_response_content)
                if audio_data:
                    st.audio(audio_data)
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": assistant_response_content,
                        "audio": audio_data
                    })
                else:
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": assistant_response_content
                    })

    # 음성 인식 (Web Speech API, JavaScript -> Streamlit)
    components.html(
        f"""
<script>
    // 함수들을 전역 범위에 정의
    window.startRecognition = function() {{
        const recognition = new window.webkitSpeechRecognition(); // Chrome
        recognition.continuous = false;
        recognition.interimResults = false;
        recognition.lang = 'ko-KR';

        recognition.onresult = (event) => {{
            const result = event.results[0][0].transcript;
            // Streamlit에 음성 인식 결과 전달 (세션 상태 사용)
            const set_user_input_from_speech = (value) => {{
               window.top.Streamlit.setSessionState({{"user_input_from_speech": value}});
            }}
             set_user_input_from_speech(result);
        }};

        recognition.onerror = (event) => {{
            console.error('Speech recognition error:', event.error);
            // 필요한 경우 에러 처리 로직 추가
        }};

        recognition.onend = () => {{
            console.log("ended");
            // 음성 인식이 종료된 후 필요한 로직 (예: 버튼 상태 업데이트)
              if (window.Streamlit) {{
                window.Streamlit.setSessionState({{"speech_active": false}});
                }}
        }};

        recognition.start();
        console.log('Speech recognition started');
         window.Streamlit.setSessionState({{"recognition": recognition}});
    }};

    window.stopRecognition = function() {{
        if (window.Streamlit.sessionState.recognition) {{
            window.Streamlit.sessionState.recognition.stop();
            window.Streamlit.setSessionState({{"recognition": null}}); //객체 저장 해제
        }}
    }};


    // 음성 인식 시작/중지 로직
    if ({'true' if st.session_state.speech_active else 'false'}) {{
        if (!window.Streamlit.sessionState.recognition) {{ // recognition 객체가 없으면 시작
            window.startRecognition();
        }}
    }} else {{
        window.stopRecognition();
    }}


</script>
""",
        height=0,
    )

if __name__ == "__main__":
    asyncio.run(main())
